{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing\n",
    "\n",
    "Lindsey Oberhelman and Niklas Tillenburg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the LSH algorithm and apply it to the data.\n",
    "•Tune it (signature length, number of bands, number of rows per band)•Randomize, optimize, benchmark, polish the code, •Deliver your code, including lines which:\n",
    "1. load the file user_movie.npy(don’t include this file in your submission!)\n",
    "2. dump results to a text file ans.txt(just a csv list of records: user1, user2)\n",
    "3. set the random seed to a specific value, eg.: np.random.seed(seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "import resource\n",
    "import time as time\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matrix():\n",
    "    \"\"\"\n",
    "    Loads the data and make a sparse matrix using the scipy module. \n",
    "    Every user movie pair in the file is represented as a '1'.\n",
    "    \"\"\"\n",
    "    # The format of the file is Col1: Users  Col2: Movies\n",
    "    data = np.load('user_movie.npy')\n",
    "\n",
    "    # Get the total amount of users and total amount of movies rated\n",
    "    users = data[:, 0]\n",
    "    movies = data[:, 1]\n",
    "\n",
    "    # find the largest user and movie ID and since python starts at 0 need add 1 \n",
    "    number_users = np.max(users)+1\n",
    "    number_movies = np.max(movies)+1\n",
    "   \n",
    "    # if a user rated a movie put into the sparse matrix a '1'\n",
    "    mat_values = np.ones(data.shape[0])\n",
    "\n",
    "    # This saves memory and makes Jaccard calculation easier\n",
    "    sparse_matrix = sparse.csc_matrix((mat_values, (movies, users)), shape=(number_movies, number_users), dtype='b')\n",
    "\n",
    "    return sparse_matrix, number_users, number_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minhashing\n",
    "\n",
    "Minhashing takes a random permutation of the column of movies and hashes the index of the first row for each user in which the column has a value 1. This is repeated for n permutations to construct a minihash signature for each user. These are then combined for all users to construct a signature matrix that will have the same number of columns as there are users and as many rows as permutations n were selected.\n",
    "As the data sets become extremely large, minihasing can become computationally expensive for large datasets as each permutation requires the dataset to be sorted. The big O notation O(n$^{2}$) where n is the number of things you are comparing. This can be infeasible or even impossible to achieve on extremely large datasets. However since we will combine this with LSH algorithm it will be a bit faster.\n",
    "Our code:\n",
    "<br>\n",
    "\n",
    "Take a random permuatation of the column of movies (new_row_permutation)\n",
    "<br>\n",
    "Then the code locates the first position where the user rated a movie records that movie number so each column is a user and each row of that column is a different permuation of movies.\n",
    "<br>\n",
    "Our code repeats that for 100 permuations becuase when we tested for 80, 100, 120. A signature matrix was the best we could do in terms of the time. As we mentioned minihashing can become expensive. In this set up it makes the signature in under 3 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signatures(S, number_users, number_movies, random_seed):\n",
    "    \"\"\"\n",
    "    Create the signature matrix using minhashing. By usage of random permutations of the rows of the sparse\n",
    "    data matrix.\n",
    "    num_permutations: 100 from the text book\n",
    "    Inputs: The sparse matrix, the number of users, the number of movies, a random seed\n",
    "    \"\"\"\n",
    "    num_permutations = 100\n",
    "\n",
    "    # allocate memory, force to int16\n",
    "    sign_matrix = np.zeros((num_permutations, number_users)).astype('int16')\n",
    "    \n",
    "    # now get the 100 permutations for the signatures\n",
    "    for perm in range(num_permutations):\n",
    "        \n",
    "        # set the random seed such that our results are the same but also enables different permutations\n",
    "        r_seed = int(perm * random_seed)\n",
    "        np.random.seed(r_seed)\n",
    "\n",
    "        # create a new random permutation of the movies\n",
    "        new_row_permutation = np.random.permutation(number_movies)\n",
    "\n",
    "        # Swap the sparse matrix rows with the new row permuation\n",
    "        perm_sparse = S[new_row_permutation, :]\n",
    "\n",
    "        # Get the position of the first '1' in each column (user) and save the position of it in the signature matrix\n",
    "        for j in range(number_users):\n",
    "            \n",
    "            #Hashing\n",
    "            position = perm_sparse.indices[perm_sparse.indptr[j]:perm_sparse.indptr[j + 1]].min()\n",
    "            sign_matrix[perm, j] = position\n",
    "\n",
    "    return sign_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity\n",
    "\n",
    "The Jaccard similarity calculates the similarity of two users by taking their intersection, the number of movies they have both watched, divided by their union, the total number of movies they have watched combined. The larger the Jaccard similarity the more probable it is that the two users are similar. We use the Jaccard Similarity to determine unique and candidate pairs and to determine true pairs in the final check as well. For the signature matrix we set the Jaccard similarity limit to be 0.485 because it allows more false negatives to pass through to the final check. When we tested our code with 0.5 for the signature similarity check and the sparse matrix check we found that there were many more false negatives and our code only found 430 pairs and we still had time left so we tuned algorithm to allow more potentially similar candidate pairs through to the final check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def j_sim_signature(user1, user2, sign_matrix):\n",
    "    \"\"\"    \n",
    "    This function calculates the similarity between two users (user1 and user2) using the signature matrix   \n",
    "    Inputs: The two users that need to compare, and the signature matrix\n",
    "    \"\"\"\n",
    "    union = len(sign_matrix[:, user1])\n",
    "    intersection = float(np.count_nonzero(sign_matrix[:, user1] == sign_matrix[:, user2]))\n",
    "    similar = intersection/union\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing (LSH)\n",
    "\n",
    "Now that Minhashing has reduced the size of the data, LSH generates hash codes that have the property of being similar for near by users. One general approach to LSH is to “hash” items so that similar items are more likely to be hashed to the same \"bucket\" than dissimilar items. Then it takes any pair that hashed to the same bucket and checks only those pairs for true similarity. This algorithm works on the hope that that most of the dissimilar pairs will not be hased to the same bucket. False positives are those dissimilar pairs that do hash to the same bucket. The algorithm also aims to limit the number of false negatives or pairs that are similar and do not get hashed to the same bucket.  The algortihm first divides the signature matrix into bands consisting of r rows each. Each band,has a hash function that takes vectors of r integers(the portion of one column within that band) and hashes them to some large number of buckets. The algorithm uses the same hash function for all the bands, but a separate bucket array for each band, so columns with the same vector in different bands will not hash to the same bucket.\n",
    "<br>\n",
    "For our code we:\n",
    "<br>\n",
    "First we create a list of candidate pairs and we whose similarity must be measured. \n",
    "<br>\n",
    "All pairs that are not candidates are assumed to be not similar\n",
    "<br>\n",
    "We perform LSH on our signature matrix by creating a large number of hash fucntions\n",
    "<br>\n",
    "for each hash function we hash columns to buckets in which everything in the bucket is a candidate pair\n",
    "A pair becomes a candidate pair when the hash function puts one or more of the signatures in the bucket\n",
    "Want each bucket to have relatively few candidate pairs in them\n",
    "Can't use too many buckets becuase then pairs that are not truly similar will not end up in the same bucket\n",
    "\n",
    "\n",
    "We determined the number of (b) bands and (r) rows by experiementing with differnet configurations 100/20/5 (like in the book), 100/10/10, 100/25/4. \n",
    "We looked at the probabilities:\n",
    "\n",
    "Recall that the probability of two documents having the same MinHash value on any row is exactly J(A,B) and every row is independent of the other.\n",
    "<br>\n",
    "The probability of the two users haveing the same MinHash values on any of the r rows of a given band is therefore $J(A,B)^{r}$  \n",
    "<br>\n",
    "The probability that two documents have atleast one different Minhash on any given row is 1-J(A,B)$^{r}$\n",
    "<br>\n",
    "Then the probability of the teo users having atleast one different Minhash value on any of the bands  is (1-J(A,B)$^{r})^{b}$\n",
    "<br>\n",
    "From this we can determine that the probability that we need to hash the two users is 1-(1-J(A,B)$^{r})^{b}$\n",
    "<br>\n",
    "<br>\n",
    "We used the probability that we need to hash the two users to determine the best number of rows and bands by making probability distributions of different configurations. We determined that 100/25/4 was the best configuration to allow for the maximum correct pairs and to minimize the number of false negatives and false positives. However our code is not efficient enough to run this configuration in that time. When we ran it on our computer we found 790 pairs in 30 minutes. We instead choose to use the 100/20/5 configuration as it finshed running under the mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(sign_matrix, total_permutation = 100):\n",
    "    \"\"\"\n",
    "    Need a function to perform the lsh algorithm like in \n",
    "    3.4.1 in book signature length of 100, with 20 bands 5 rows like in the book.\n",
    "    http://localhost:8888/notebooks/Desktop/AiDM/lsh.ipynb#d \n",
    "    Inputs: Signature matrix, total number of permutations \n",
    "    \"\"\"\n",
    "    # buckets for sigs to be hashed into LSH with banding\n",
    "    number_bands = 20\n",
    "    number_rows = 5\n",
    "\n",
    "    # each bucket as a list, it is a list because of the different size bucket tuples\n",
    "    buckets = []\n",
    "\n",
    "    current_row = 0\n",
    "    \n",
    "    for b in range(number_bands):\n",
    "        \n",
    "        # Make the band \n",
    "        band = sign_matrix[current_row:int(number_rows) + current_row, :]\n",
    "        current_row += int(number_rows)\n",
    "\n",
    "        # obtain a array in which duplicate vectors in the band are put into the same bucket; the candidate pairs\n",
    "        indices = np.ravel_multi_index(band.astype(int), band.max(1).astype(int) + 1)\n",
    "        sidx = indices.argsort()\n",
    "        sorted_indices = indices[sidx]\n",
    "        buckets_array = np.array(np.split(sidx, np.nonzero(sorted_indices[1:] > sorted_indices[:-1])[0] + 1))\n",
    "\n",
    "        # Only record the buckets with more than 1 user in it\n",
    "        for position in range(len(buckets_array)):\n",
    "            if len(buckets_array[position]) > 1:\n",
    "                \n",
    "                buckets.append(buckets_array[position])\n",
    "\n",
    "    x = map(tuple, buckets)\n",
    "    \n",
    "    buckets = set(x)\n",
    " \n",
    "    buckets = list(buckets)\n",
    "    return buckets\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_sets(bucket_list, sign_matrix):\n",
    "    \"\"\"\n",
    "    This function find the unique candidate pairs from the hashed buckets and determines \n",
    "    if they are similar using the jaccard similarity. If it is larger then 0.5 for the signature \n",
    "    matrix then it is counted. Removes the potentail doubles.\n",
    "    Inputs: list of buckets and the signature matrix\n",
    "    \"\"\"\n",
    "\n",
    "    unique_set = set()\n",
    "    \n",
    "    for b in range(len(bucket_list)):\n",
    "        \n",
    "        # Generate all the combinations of the pairs per bucket\n",
    "        large_user_pair = set(pair for pair in itertools.combinations(bucket_list[b], 2))\n",
    "        \n",
    "        # See if they are already in the set\n",
    "        large_user_pair = large_user_pair.difference(unique_set)\n",
    "        \n",
    "        for user_pair in large_user_pair:\n",
    "            \n",
    "            similarity = j_sim_signature(user_pair[0], user_pair[1], sign_matrix)\n",
    "            \n",
    "            if similarity >= 0.4:\n",
    "                unique_set.add(user_pair)\n",
    "                \n",
    "    return unique_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccards_similarity(user1, user2, sparse_matrix):\n",
    "    \"\"\"     \n",
    "    Calculate and return the Jaccard similarity between two users (user1 and user2) with the sparse matrix   \n",
    "    Inputs: Two unique users and the sparese matrix\n",
    "    \"\"\"\n",
    "    intersection = np.sum(sparse_matrix[:, user1] & sparse_matrix[:, user2])\n",
    "    union = np.sum(sparse_matrix[:, user1] | sparse_matrix[:, user2])\n",
    "    jacard_sim = float(intersection) / float(union)\n",
    "    return jacard_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the unique pairs are determined by calculating the jaccard similarity on the signature matrix then we check the two users using the original sparse matrix. We first make a copy of the original matrix so that we do not edit it. Then we run thorugh the unique sets (unique_sets) and check the jaccard similarity and check for doubles. This ensures that we are getting correct pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_true_similarity(unique_sets, sparse):\n",
    "    \"\"\"\n",
    "    This function checks the jaccard similarity of each unique pair using the entire movie data\n",
    "    for the users in the sparse Matrix\n",
    "    Inputs: The set of unique pairs, the sparse matrix\n",
    "    \"\"\"\n",
    "\n",
    "    copy_unique_sets = unique_sets \n",
    "    sorted_unique_sets = sorted(copy_unique_sets)\n",
    "    \n",
    "    #Need to make the sparse matrix into an array in order for the | and & to work\n",
    "    original_matrix = sparse.toarray()\n",
    "    pair_list = []\n",
    "    \n",
    "    # check if the similarity is really > 0.5 with the jaccard similarity \n",
    "    for pairs in unique_sets:\n",
    "        \n",
    "        #This ensures that false positives and doubles are taken care of.\n",
    "        # if user1 < user2 and j add to the list of definite pairs.\n",
    "        if pairs[0] < pairs[1]:\n",
    "            \n",
    "            sim = jaccards_similarity(pairs[0], pairs[1], original_matrix)\n",
    "            \n",
    "            # and jaccard similarity > 0.5\n",
    "            if sim > 0.5:\n",
    "                \n",
    "                pair_list.append((pairs[0], pairs[1]))\n",
    "\n",
    "        # if user 2 is larger than user 1  \n",
    "        elif pairs[0] > pairs[1]:\n",
    "            \n",
    "            # check to see if pair is in the set \n",
    "            if (pairs[1], pairs[0]) in copy_unique_sets:\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            # if not then calcualte the jaccard and save it\n",
    "            else:\n",
    "                \n",
    "                sim = jaccards_similarity(pairs[0], pairs[1], original_matrix)\n",
    "                \n",
    "                if sim > 0.5:\n",
    "                \n",
    "                    pair_list.append((pairs[1], pairs[0]))\n",
    "\n",
    "    #sort user pair list so that it is easy to compare to all_the_pairs.txt\n",
    "    pair_list = sorted(pair_list)\n",
    "    \n",
    "    # write to txt file\n",
    "    with open('results.txt', 'w') as f:\n",
    "        f.write('\\n'.join('%s,%s' % user_pair for user_pair in pair_list))\n",
    "    f.close()\n",
    "    print('User Pair: ', len(pair_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with signature\n",
      "checking the true similarity of the pairs\n",
      "User Pair:  655\n",
      "The total time in minutes to run is:5.942305966218313\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    This funciton: Starts the timer, Loads the data, Creates the sparse matrix,\n",
    "    Makes the signatures and hashing them into buckets, Then find similar signatures and\n",
    "    checks the jaccard similarity with the sparse matrix, Finally creates the file of pairs\n",
    "    \"\"\"\n",
    "    #Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Import the data and make the sparse matrix\n",
    "    S,u,m = sparse_matrix()\n",
    "   \n",
    "    #set a random seed for the permuation of movies and make the signature matrix\n",
    "    signature_matrix = make_signatures(S, u, m, 1234) \n",
    "    print('done with signature')\n",
    "    \n",
    "    \n",
    "    #hash the signatures to buckets\n",
    "    buckets = lsh(signature_matrix)\n",
    "    #print('The number of buckets is {}'.format(len(buckets)))\n",
    "    \n",
    "    #Find the unique pairs using j similarity on the signature matrix\n",
    "    unique_set = find_unique_sets(buckets, signature_matrix)\n",
    "    #print('The number of unique pairs is {}'.format(len(unique_set)))\n",
    "    \n",
    "    #Checks the user pairs similarity on the sparse matrix\n",
    "    print('checking the true similarity of the pairs')\n",
    "    check_true_similarity(unique_set, S)\n",
    "    total_time = (time.time()-start_time)/60\n",
    "    print(\"The total time in minutes to run is:{}\".format(total_time))\n",
    "\n",
    "main()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results\n",
    "On out laptops our code produces \n",
    "\n",
    "430 pairs in about 5 minutes. \n",
    "566 pairs in about 6 minutes.\n",
    "655 pairs in about 9 minutes.\n",
    "700+ in over 15 minutes.\n",
    "\n",
    "All of them were correct. We checked our results against the all_similar_pairs.txt in the OLD assignment folder and we found that most of our pairs are on that list. So we know that our signature we are atleast generating correct pairs. We suspect that if we perform more hashings we will get more pairs however our code is not efficient enough to accomplish this in 15 minutes. The ideal ratio of bands to rows is 100 permutations , 25 bands, and 4 rows. We discovered this doing the research into the probability of pairs and making the plots. Below you can see the comparison between our results and the file of all the similar pairs from last year and all of our pairs are in the results from last year so we are generating correct pairs. \n",
    "<br>\n",
    "## Computaional Expenses\n",
    "Time: Our code ran in about five minutes on a average laptop. The time compexity for this code was a bit hard to determine. For the loading of the data loading the data seemed to scale with the size of the file. Creating the signatire matrix scaled with O(n$*u$) where n is the number of permuations you want and u is the number of users. And since the code loops through the the users for every number of permuations. Then for the lsh function it scaled with the number of bands and rows we decided to go with O(r$*b).$ For the final check O($s*sparse$) It was the number of sets and then the time it took to access all the data in the sparse matrix. \n",
    "<br>\n",
    "Memory: We conserve memory in our algorithm by using a sparse matrix, which deletes all the zeros and only saves the movies each user has seen as ones. This cuts out a lot of memory that in such a huge data set is crucial. The memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results =  np.loadtxt('results.txt', delimiter = ',')\n",
    "pairs = np.loadtxt('all_similar_pairs.txt', delimiter = ',')\n",
    "pairs = pairs[:,:2]\n",
    "any(np.setdiff1d(results, pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "Ways to imporve the code would be to add a transitive property where if user 1 and user 2 are both similar to user 3 then they are similar. This would improve the speed and limit the number of checks. If we are also able to make the code faster we could generate more unique pairs from the signature by lowering the jaccard similarity limit and being able to complete it in time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
